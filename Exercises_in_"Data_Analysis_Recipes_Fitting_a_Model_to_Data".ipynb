{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercises in \"Data Analysis Recipes: Fitting a Model to Data\".ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "yym10bj1Skv-",
        "uyCYT3cAShRD",
        "FKGFuFf2RR3J",
        "KUOcMPqI7x9H",
        "tZkeLcN68Bla",
        "M7CHbWqcGZdn",
        "undPLXEqutoM",
        "oF713atMEhxf",
        "T-CKWeD15x4a",
        "QKtoSCnR5zMe",
        "Pl1-Xn_t17n2",
        "vfj57FfEH8x-",
        "9U68vtXaLsYL",
        "QNEW4KbyLzUI",
        "jzEGe6ycdaUM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdconinc/data-analysis-recipes-fitting-a-model-to-data/blob/master/Exercises_in_%22Data_Analysis_Recipes_Fitting_a_Model_to_Data%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pc2JRYQhRcWR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Analysis Recipes: Fitting a Model to Data\n",
        "\n",
        "This notebook works through the exercises in [arXiv:1008.4686v1](https://arxiv.org/abs/1008.4686) [astro-ph.IM] \"Data Analysis Recipes: Fitting a Model to Data\", by David W. Hogg, Jo Bovy, and Dustin Lang, hereafter the \"paper\".\n",
        "\n",
        "Copyright of the material in the paper belongs to the original authors (see footnote 1 in the paper for license information). Fair use exception (educational use, excerpt) justifies copying the data from Table 1 into this notebook. Fair use exception (educational use, excerpt) justifies reproduction of some formulas for implementation in this notebook.\n",
        "\n",
        "The license of the solutions to the paper's exercises in this notebook is Creative Commons Attribution Share Alike 4.0, CC BY-SA 4.0. Original author is Wouter Deconinck, William & Mary."
      ]
    },
    {
      "metadata": {
        "id": "yym10bj1Skv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preamble"
      ]
    },
    {
      "metadata": {
        "id": "ZnRA_QfcSkfs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize as opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uyCYT3cAShRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "PzCo-9tESgwp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array([201, 244, 47, 287, 203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146], dtype = float)\n",
        "Y = np.array([592, 401, 583, 402, 495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344], dtype = float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CA4eCRaLTXAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sigma_Y = np.array([61, 25, 38, 15, 21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22], dtype = float)\n",
        "sigma_X = np.array([9, 4, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 5, 6, 6, 5, 9, 8, 6, 5], dtype = float)\n",
        "rho_XY = np.array([-0.84, 0.31, 0.64, -0.27, -0.33, 0.67, -0.02, -0.05, -0.84, -0.69, 0.30, -0.46, -0.03, 0.50, 0.73, -0.52, 0.90, 0.40, -0.78, -0.56], dtype = float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZoRwNrjYVtHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No correlations $\\rho_{xy}$ are shown in the plot below."
      ]
    },
    {
      "metadata": {
        "id": "ISxbswCVwkZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.errorbar(X, Y, xerr = sigma_X, yerr = sigma_Y, marker = \".\", linestyle = \"\");\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKGFuFf2RR3J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Analytical linear least squares without outliers"
      ]
    },
    {
      "metadata": {
        "id": "tYTLvZGORURU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y1 = Y[4:]\n",
        "A1 = np.column_stack([np.ones_like(X[4:]), X[4:]])\n",
        "C1 = np.diag(sigma_Y[4:]**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LrpN515P4Yhn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X1 = inv(A1.T @ inv(C1) @ A1) @ (A1.T @ inv(C1) @ Y1)\n",
        "S1 = inv(A1.T @ inv(C1) @ A1)\n",
        "print(\"b + m x = (%f +/- %f) + (%f +/- %f) x\" % (X1[0], np.sqrt(S1[0][0]), \n",
        "                                                 X1[1], np.sqrt(S1[1][1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZqHAZJkCjAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.errorbar(X[4:], Y[4:], yerr = sigma_Y[4:], marker = \".\", linestyle = \"\");\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);\n",
        "x = np.linspace(0, 300)\n",
        "A = np.column_stack([np.ones_like(x), x])\n",
        "y = A @ X1\n",
        "plt.plot(x, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUOcMPqI7x9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Analytical linear least squares with outliers"
      ]
    },
    {
      "metadata": {
        "id": "Q0nAJLOv74wi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y2 = Y\n",
        "A2 = np.column_stack([np.ones_like(X), X])\n",
        "C2 = np.diag(sigma_Y**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZKGt9m27xgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X2 = inv(A2.T @ inv(C2) @ A2) @ (A2.T @ inv(C2) @ Y2)\n",
        "S2 = inv(A2.T @ inv(C2) @ A2)\n",
        "print(\"b + m x = (%f +/- %f) + (%f +/- %f) x\" % (X2[0], np.sqrt(S2[0][0]),\n",
        "                                                 X2[1], np.sqrt(S2[1][1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcKsnECj7-0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.errorbar(X, Y, yerr = sigma_Y, marker = \".\", linestyle = \"\");\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);\n",
        "x = np.linspace(0, 300)\n",
        "A = np.column_stack([np.ones_like(x), x])\n",
        "y = A @ X2\n",
        "plt.plot(x, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZkeLcN68Bla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Analytical quadratic solution with outliers"
      ]
    },
    {
      "metadata": {
        "id": "N2Or7dBW8CbE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y3 = Y[4:]\n",
        "A3 = np.column_stack([np.ones_like(X[4:]), X[4:], X[4:]**2])\n",
        "C3 = np.diag(sigma_Y[4:]**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9W_W2Fxi8HW4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X3 = inv(A3.T @ inv(C3) @ A3) @ (A3.T @ inv(C3) @ Y3)\n",
        "S3 = inv(A3.T @ inv(C3) @ A3)\n",
        "print(\"b + m x + q x**2 = (%f +/- %f) + (%f +/- %f) x + (%f +/- %f) x**2\" % (X3[0], np.sqrt(S3[0][0]),\n",
        "                                                                             X3[1], np.sqrt(S3[1][1]),\n",
        "                                                                             X3[2], np.sqrt(S3[2][2])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "duaAGsOP8HzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.errorbar(X[4:], Y[4:], yerr = sigma_Y[4:], marker = \".\", linestyle = \"\");\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);\n",
        "x = np.linspace(0, 300)\n",
        "A = np.column_stack([np.ones_like(x), x, x**2])\n",
        "y = A @ X3\n",
        "plt.plot(x, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7CHbWqcGZdn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Maximum likelihood for single value with normal measurement noise"
      ]
    },
    {
      "metadata": {
        "id": "FdH2Zt8jrBN7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The frequency distribution for $t_i$ is:\n",
        "$$ p(t_i | \\sigma_{ti}, T) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ti}^2}} \\exp\\left( -\\frac{[t_i - T]^2}{2\\sigma_{ti}^2} \\right). $$\n",
        "We can use this to calculate the likelihood\n",
        "$$ \\mathcal{L} = \\prod_{i=1}^N p(t_i | \\sigma_{ti}, T), $$\n",
        "and log likelihood \n",
        "$$ \\ln \\mathcal{L} = K - \\sum_{i=1}^N \\frac{[t_i - T]^2}{2\\sigma_{ti}^2}. $$\n",
        "The maximum log likelihood is obtained when the derivative to $T$ is zero:\n",
        "$$ 0 = \\frac{d}{dT} \\ln \\mathcal{L} = \\sum_{i=1}^N \\frac{[t_i - T]}{\\sigma_{ti}^2}. $$\n",
        "Indeed this is a maximum:\n",
        "$$ \\frac{d}{dT} \\ln \\mathcal{L} = - \\sum_{i=1}^N \\frac{1}{\\sigma_{ti}^2} < 0. $$\n",
        "The maximum log likelihood is obtained when:\n",
        "$$ T = \\frac{\\sum_{i=1}^N \\frac{t_i}{\\sigma_{ti}^2}}{\\sum_{i=1}^N \\frac{1}{\\sigma_{ti}^2}}, $$\n",
        "or an expression given by the weighted mean."
      ]
    },
    {
      "metadata": {
        "id": "undPLXEqutoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Derivation of analytical expression"
      ]
    },
    {
      "metadata": {
        "id": "zDMhiBlzuvTH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Starting from the definition of $\\chi^2$ as\n",
        "$$ \\chi^2 = [\\mathbf{Y} - \\mathbf{A} \\mathbf{X}]^T \\mathbf{C}^{-1} [\\mathbf{Y} - \\mathbf{A} \\mathbf{X}] $$\n",
        "we can take a derivative with respect to $\\mathbf{X}$ as\n",
        "$$ 0 = \\frac{d}{d\\mathbf{X}} \\chi^2 = -\\mathbf{A}^T \\mathbf{C}^{-1} [\\mathbf{Y} - \\mathbf{A} \\mathbf{X}] -  [\\mathbf{Y} - \\mathbf{A} \\mathbf{X}]^T \\mathbf{C}^{-1} \\mathbf{A} $$\n",
        "or\n",
        "$$ \\mathbf{A}^T \\mathbf{C}^{-1} \\mathbf{A} \\mathbf{X} = \\mathbf{A}^T \\mathbf{C}^{-1} \\mathbf{Y} $$\n",
        "which is satisfied for\n",
        "$$ \\mathbf{X} = [\\mathbf{A}^T \\mathbf{C}^{-1} \\mathbf{A}]^{-1} [\\mathbf{A}^T \\mathbf{C}^{-1} \\mathbf{Y}] $$"
      ]
    },
    {
      "metadata": {
        "id": "oF713atMEhxf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises 6, 7: Mixture model using Metropolis-Hastings MCMC"
      ]
    },
    {
      "metadata": {
        "id": "CWgU4DfCgEzd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The (unnormalized) likelihood $\\mathcal{L}$ in (17) in the paper can be implemented:"
      ]
    },
    {
      "metadata": {
        "id": "hMoB2KgJw1Pp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def likelihood(x, y, sigma, b, m, Pb = 0.0, Yb = 0.0, Vb = 0.0):\n",
        "  return np.prod( (1-Pb)* 1/np.sqrt(2*np.pi*sigma**2)        * np.exp(-(y-m*x-b)**2/(2*sigma**2))\n",
        "                 + Pb   * 1/np.sqrt(2*np.pi*(Vb + sigma**2)) * np.exp(-(y-Yb)**2   /(2*(Vb + sigma**2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bKCunb8gOTX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, it makes more sense for reasons of numerical stability to use $\\ln\\mathcal{L}$, which turns the product into a sum of logs:"
      ]
    },
    {
      "metadata": {
        "id": "lyP_RJ49Ph3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_likelihood(x, y, sigma, b, m, Pb = 0.0, Yb = 0.0, Vb = 0.0):\n",
        "  return np.sum( np.log( (1-Pb)* 1/np.sqrt(2*np.pi*sigma**2)        * np.exp(-(y-m*x-b)**2/(2*sigma**2))\n",
        "                        + Pb   * 1/np.sqrt(2*np.pi*(Vb + sigma**2)) * np.exp(-(y-Yb)**2/(2*(Vb + sigma**2)))))\n",
        "\n",
        "b = 40.0\n",
        "m = 2.2\n",
        "np.testing.assert_almost_equal(np.exp(log_likelihood(X, Y, sigma_Y, b, m)), likelihood(X, Y, sigma_Y, b, m))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pq8nN3GOhHWU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We first try to find the maximum likelihood for $P_b$ equal to zero, which should revert to the solutions of Exercise 1 (ignoring outliers) and Exercise 2 (including outliers)."
      ]
    },
    {
      "metadata": {
        "id": "T-CKWeD15x4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reproducing Exercise 1: Analytical linear least squares without outliers"
      ]
    },
    {
      "metadata": {
        "id": "-yb2qK1Ta5yR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b = np.linspace(-125.0, +125.0, 100)\n",
        "m = np.linspace(1.5, 3.1, 100)\n",
        "\n",
        "L = np.ndarray((len(m), len(b)))\n",
        "for i in range(len(m)):\n",
        "  for j in range(len(b)):\n",
        "    L[i,j] = log_likelihood(X[4:], Y[4:], sigma_Y[4:], b[j], m[i])\n",
        "\n",
        "plt.contour(b, m, L, 100)\n",
        "\n",
        "x0 = np.array([40.0, 2.2])\n",
        "res = opt.minimize(lambda x: -log_likelihood(X[4:], Y[4:], sigma_Y[4:], x[0], x[1]), x0)\n",
        "plt.plot(res.x[0], res.x[1], 'o')\n",
        "plt.xlabel(\"$b$\"); plt.ylabel(\"$m$\");\n",
        "print(\"b + m x = (%f +/- %f) + (%f +/- %f) x\" % (res.x[0], '?'),\n",
        "                                                 res.x[1], '?'))\n",
        "print(\"chi^2(b = %f, m = %f) = %f\" % (res.x[0], res.x[1], res.fun))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHK7yPdzYRKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: We could get the covariance matrix from the Jacobian: $\\mathbf{C} \\propto \\mathbf{J}^T \\mathbf{J}$"
      ]
    },
    {
      "metadata": {
        "id": "QKtoSCnR5zMe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reproducing Exercise 2: Analytical linear least squares with outliers"
      ]
    },
    {
      "metadata": {
        "id": "C9_xzqV3rTpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b = np.linspace(-125.0, +225.0, 100)\n",
        "m = np.linspace(1.0, 2.5, 100)\n",
        "\n",
        "L = np.ndarray((len(m), len(b)))\n",
        "for i in range(len(m)):\n",
        "  for j in range(len(b)):\n",
        "    L[i,j] = log_likelihood(X, Y, sigma_Y, b[j], m[i])\n",
        "\n",
        "plt.contour(b, m, L, 100)\n",
        "\n",
        "x0 = np.array([40.0, 2.2])\n",
        "res = opt.minimize(lambda x: -log_likelihood(X, Y, sigma_Y, x[0], x[1]), x0)\n",
        "plt.plot(res.x[0], res.x[1], 'o')\n",
        "plt.xlabel(\"$b$\"); plt.ylabel(\"$m$\");\n",
        "print(\"b + m x = (%f +/- %f) + (%f +/- %f) x\" % (res.x[0], '?'),\n",
        "                                                 res.x[1], '?'))\n",
        "print(\"chi^2(b = %f, m = %f) = %f\" % (res.x[0], res.x[1], res.fun))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vWfNi1H4am33"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: We could get the covariance matrix from the Jacobian: $\\mathbf{C} \\propto \\mathbf{J}^T \\mathbf{J}$"
      ]
    },
    {
      "metadata": {
        "id": "Pl1-Xn_t17n2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Metropolis-Hastings Markov Chain Monte Carlo"
      ]
    },
    {
      "metadata": {
        "id": "IFvZIVRVduwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We implement the Metropolis-Hasting Markov Chain Monte Carlo algorithm to sample the posterior distribution. For inspiration on how to implement this, see [Joseph Moukarzel's From Scratch post](https://towardsdatascience.com/from-scratch-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings-in-python-ef21a29e25a). "
      ]
    },
    {
      "metadata": {
        "id": "gs9TybXhRAkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def metropolis_hastings(log_likelihood, acceptance, prior, transition, x0, iterations = 1000):\n",
        "    x_old = x0\n",
        "    accepted = []\n",
        "    rejected = []   \n",
        "    for i in range(iterations):\n",
        "        x_new = transition(x_old)\n",
        "        L_old = log_likelihood(x_old)\n",
        "        L_new = log_likelihood(x_new) \n",
        "        if (acceptance(L_old + np.log(prior(x_old)), L_new + np.log(prior(x_new)))):            \n",
        "            x_old = x_new\n",
        "            accepted.append(x_new)\n",
        "        else:\n",
        "            rejected.append(x_new)\n",
        "\n",
        "    return np.array(accepted), np.array(rejected)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o549SXf-nVcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now apply this with the following conditions:\n",
        "\n",
        "* an acceptance criterion if $U(0,1) < \\exp(\\mathcal{L}_{new} - \\mathcal{L}_{old})$,\n",
        "* an uninformative prior that is flat for $P_b$ in $[0,1]$, and logarithmic in $V_b > 0$,\n",
        "* a normal step distribution with 'reasonable' widths.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "icMv4TMK6nnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x0 = np.array([34., 2.2, 0.0, 0.0, 0.0])\n",
        "s0 = np.array([10.0, 0.1, 0.2, 100.0, 100.0])\n",
        "iterations = 1000000\n",
        "accepted, rejected = metropolis_hastings(lambda x: log_likelihood(X, Y, sigma_Y, x[0], x[1], x[2], x[3], x[4]),\n",
        "                                         lambda L_old, L_new: (np.random.uniform(0,1) < np.exp(L_new - L_old)), \n",
        "                                         lambda x: 0.0 if (x[2] <= 0 or x[2] >= 1 or x[4] <= 0) else 1.0 / x[4],\n",
        "                                         lambda x: np.random.normal(x, s0),\n",
        "                                         x0, iterations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIba20F-CgXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(100.0 * accepted.shape[0] / (accepted.shape[0] + rejected.shape[0]), \"% acceptance rate (target is 50%)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uALz3MuPFtAt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,20))\n",
        "ax = fig.add_subplot(3,1,1)\n",
        "ax.plot(rejected[:50,0], rejected[:50,1], 'rx', label = 'Rejected')\n",
        "ax.plot(accepted[:50,0], accepted[:50,1], 'b.', label = 'Accepted')\n",
        "ax.plot(accepted[:50,0], accepted[:50,1], label = \"Path\", alpha = 0.5)\n",
        "ax.set_xlabel(\"b\");\n",
        "ax.set_ylabel(\"m\");\n",
        "ax.set_xlim(-125.0, +125.0);\n",
        "ax.set_ylim(1.5, 3.1);\n",
        "ax.legend()\n",
        "ax.set_title(\"MCMC sampling for $m$ and $b$ with Metropolis-Hastings. First 50 samples are shown.\"); \n",
        "\n",
        "ax = fig.add_subplot(3,1,2)\n",
        "ax.plot(rejected[:,0], rejected[:,1], 'rx', label = 'Rejected', alpha = 0.3)\n",
        "ax.plot(accepted[:,0], accepted[:,1], 'b.', label = 'Accepted', alpha = 0.3)\n",
        "ax.plot(accepted[:,0], accepted[:,1], label = \"Path\", alpha = 0.5)\n",
        "ax.set_xlabel(\"b\");\n",
        "ax.set_ylabel(\"m\");\n",
        "ax.set_xlim(-125.0, +125.0);\n",
        "ax.set_ylim(1.5, 3.1);\n",
        "ax.legend()\n",
        "ax.set_title(\"MCMC sampling for $m$ and $b$ with Metropolis-Hastings. All samples are shown.\");\n",
        "\n",
        "to_show = 500\n",
        "ax = fig.add_subplot(3,1,3)\n",
        "ax.plot(rejected[-to_show:,0], rejected[-to_show:,1], 'rx', label = 'Rejected', alpha = 0.5)\n",
        "ax.plot(accepted[-to_show:,0], accepted[-to_show:,1], 'b.', label = 'Accepted', alpha = 0.5)\n",
        "ax.plot(accepted[-to_show:,0], accepted[-to_show:,1], label = \"Path\", alpha = 0.5)\n",
        "ax.set_xlabel(\"b\");\n",
        "ax.set_ylabel(\"m\");\n",
        "ax.set_xlim(-125.0, +125.0);\n",
        "ax.set_ylim(1.5, 3.1);\n",
        "ax.legend()\n",
        "ax.set_title(\"MCMC sampling for $m$ and $b$ with Metropolis-Hastings. Last 50 samples are shown.\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6iMf37E8OkmS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_accepted = accepted.shape[0]\n",
        "\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "ax = fig.add_subplot(5,1,1)\n",
        "plt.hist(accepted[int(0.5*num_accepted):,0], density = True, bins = 100);\n",
        "ax = fig.add_subplot(5,1,2)\n",
        "plt.hist(accepted[int(0.5*num_accepted):,1], density = True, bins = 100);\n",
        "ax = fig.add_subplot(5,1,3)\n",
        "plt.hist(accepted[int(0.5*num_accepted):,2], density = True, bins = 100);\n",
        "ax = fig.add_subplot(5,1,4)\n",
        "plt.hist(accepted[int(0.5*num_accepted):,3], density = True, bins = 100);\n",
        "ax = fig.add_subplot(5,1,5)\n",
        "plt.hist(accepted[int(0.5*num_accepted):,4], density = True, bins = 100);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTxnwHFDXVLo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "counts, xbins, ybins, image = plt.hist2d(accepted[int(0.5*num_accepted):,0],\n",
        "                                         accepted[int(0.5*num_accepted):,1],\n",
        "                                         bins = 100)\n",
        "plt.xlabel(\"$b$\"); plt.ylabel(\"$m$\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9mql-rXjJDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contour(counts.T, extent = [xbins.min(), xbins.max(), ybins.min(), ybins.max()], linewidths = 2)\n",
        "plt.xlabel(\"b\"); plt.ylabel(\"m\")\n",
        "plt.xlim(-125.0, +125.0);\n",
        "plt.ylim(1.5, 3.1);\n",
        "plt.plot(np.mean(accepted[int(0.5*num_accepted):,0]), np.mean(accepted[int(0.5*num_accepted):,1]), '.k')\n",
        "print(, )\n",
        "print(\"b + m x = (%f +/- %f) + (%f +/- %f) x\" % (np.mean(accepted[int(0.5*num_accepted):,0]), np.std(accepted[int(0.5*num_accepted):,0])),\n",
        "                                                 np.mean(accepted[int(0.5*num_accepted):,1]), np.std(accepted[int(0.5*num_accepted):,1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37MLrla4kxqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.errorbar(X, Y, yerr = sigma_Y, marker = \".\", linestyle = \"\");\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);\n",
        "\n",
        "x = np.linspace(0, 300)\n",
        "A = np.column_stack([np.ones_like(x), x])\n",
        "for i in np.random.randint(int(0.8*num_accepted), num_accepted, 1000):\n",
        "  y = A @ np.array([accepted[i,0], accepted[i,1]])\n",
        "  plt.plot(x, y, color = 'lightsteelblue', alpha = 0.005);\n",
        "  \n",
        "y = A @ np.array([np.mean(accepted[int(0.5*num_accepted):,0]), np.mean(accepted[int(0.5*num_accepted):,1])])\n",
        "plt.plot(x, y, 'k');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfj57FfEH8x-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises 6, 7: Mixture model using PyStan"
      ]
    },
    {
      "metadata": {
        "id": "9U68vtXaLsYL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reproducing Exercise 2: Analytical linear least squares without outliers"
      ]
    },
    {
      "metadata": {
        "id": "DAtN2zE-IAN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pystan\n",
        "\n",
        "model = \"\"\"\n",
        "data {\n",
        "    int<lower=0> N;\n",
        "    vector[N] x;\n",
        "    vector[N] y;\n",
        "    vector[N] sy;\n",
        "}\n",
        "parameters {\n",
        "    real b;\n",
        "    real m;\n",
        "}\n",
        "model {\n",
        "    y ~ normal(b + m * x, sy);\n",
        "}\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZfQ6M7lIO2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Put our data in a dictionary\n",
        "data = {'N': len(X), 'x': X, 'y': Y, 'sy': sigma_Y}\n",
        "\n",
        "# Compile the model\n",
        "sm = pystan.StanModel(model_code = model)\n",
        "\n",
        "# Train the model and generate samples\n",
        "fit = sm.sampling(data = data, iter = 1000, chains = 4, warmup = 500, thin = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4liCQpR6IoVh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_dict = fit.summary()\n",
        "\n",
        "df = pd.DataFrame(summary_dict['summary'], \n",
        "                  columns = summary_dict['summary_colnames'], \n",
        "                  index = summary_dict['summary_rownames'])\n",
        "\n",
        "b_mean, m_mean = df['mean']['b'], df['mean']['m']\n",
        "\n",
        "# Extracting traces\n",
        "b = fit['b']\n",
        "m = fit['m']\n",
        "lp = fit['lp__']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXq4pXABJEVE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot mean regression line\n",
        "plt.errorbar(X, Y, yerr = sigma_Y, marker = \".\", linestyle = \"\")\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);\n",
        "\n",
        "# Plotting regression line\n",
        "x_min, x_max = 0.0, 300.0\n",
        "x_plot = np.linspace(x_min, x_max, 100)\n",
        "plt.plot(x_plot, b_mean + m_mean * x_plot, '-k')\n",
        "\n",
        "# Plot a subset of sampled regression lines\n",
        "np.random.shuffle(m), np.random.shuffle(b)\n",
        "for i in range(1000):\n",
        "  plt.plot(x_plot, b[i] + m[i] * x_plot,\n",
        "           color = 'lightsteelblue', \n",
        "           alpha = 0.005 )\n",
        "\n",
        "plt.title('Fitted Regression Line');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNEW4KbyLzUI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mixture of linear signal with normal and constant backgound with normal"
      ]
    },
    {
      "metadata": {
        "id": "N6DoXfCTL24t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See for example the [Stan user guide](https://mc-stan.org/docs/2_19/stan-users-guide/summing-out-the-responsibility-parameter.html)."
      ]
    },
    {
      "metadata": {
        "id": "A-bgOVG2Lgrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pystan\n",
        "\n",
        "model = \"\"\"\n",
        "data {\n",
        "    int<lower=0> N;\n",
        "    vector[N] x;\n",
        "    vector[N] y;\n",
        "    vector[N] sy;\n",
        "}\n",
        "parameters {\n",
        "    real b;\n",
        "    real m;\n",
        "    simplex[2] Pb;\n",
        "    real Yb;\n",
        "    real<lower=0> Vb;\n",
        "}\n",
        "model {\n",
        "    vector[2] log_Pb = log(Pb);\n",
        "    for (n in 1:N) {\n",
        "        vector[2] lps = log_Pb;\n",
        "        real mu = b + m * x[n];\n",
        "        lps[1] += normal_lpdf(y[n] | mu, sy[n]);\n",
        "        real sby = sqrt(Vb + sy[n]^2);\n",
        "        lps[2] += normal_lpdf(y[n] | Yb, sby);\n",
        "        target += log_sum_exp(lps);\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "b46sKVsxLn4F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Put our data in a dictionary\n",
        "data = {'N': len(X), 'x': X, 'y': Y, 'sy': sigma_Y}\n",
        "\n",
        "# Compile the model\n",
        "sm = pystan.StanModel(model_code = model)\n",
        "\n",
        "# Train the model and generate samples\n",
        "fit = sm.sampling(data = data, iter = 1000, chains = 4, warmup = 500, thin = 1, control = {'max_treedepth': 20})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sPolRmchLn4J",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_dict = fit.summary()\n",
        "\n",
        "df = pd.DataFrame(summary_dict['summary'], \n",
        "                  columns = summary_dict['summary_colnames'], \n",
        "                  index = summary_dict['summary_rownames'])\n",
        "\n",
        "b_mean, m_mean = df['mean']['b'], df['mean']['m']\n",
        "\n",
        "# Extracting traces\n",
        "b = fit['b']\n",
        "m = fit['m']\n",
        "lp = fit['lp__']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1JYvFz0iLn4N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot mean regression line\n",
        "plt.errorbar(X, Y, yerr = sigma_Y, marker = \".\", linestyle = \"\")\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\");\n",
        "plt.xlim(0, 300);  plt.ylim(0, 700);\n",
        "\n",
        "# Plotting regression line\n",
        "x_min, x_max = 0.0, 300.0\n",
        "x_plot = np.linspace(x_min, x_max, 100)\n",
        "plt.plot(x_plot, b_mean + m_mean * x_plot, '-k')\n",
        "\n",
        "# Plot a subset of sampled regression lines\n",
        "np.random.shuffle(m), np.random.shuffle(b)\n",
        "for i in range(1000):\n",
        "  plt.plot(x_plot, b[i] + m[i] * x_plot,\n",
        "           color = 'lightsteelblue', \n",
        "           alpha = 0.005 )\n",
        "\n",
        "plt.title('Fitted Regression Line')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzEGe6ycdaUM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises 6, 7: Full exponential model using PyStan"
      ]
    },
    {
      "metadata": {
        "id": "mGS2YXGGxEcv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Binomial prior probability on the data rejection:\n",
        "$$ p(\\{q_i\\}_{i=1}^N|P_b,I) = \\prod_{i=1}^N [1-P_b]^{q_i} P_b^{[1-q_i]} $$"
      ]
    },
    {
      "metadata": {
        "id": "PkogkosBwVkx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prior(q, Pb):\n",
        "  return np.prod( (1-Pb)**q * Pb**(1-q) )\n",
        "\n",
        "N = 10\n",
        "Pb = 0.7\n",
        "np.testing.assert_almost_equal(p(np.zeros(N),Pb), Pb**N)\n",
        "np.testing.assert_almost_equal(p(np.ones(N),Pb), (1-Pb)**N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvaFrYMcdh0c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}